{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Теоретическая чаcть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Прямой проход рекуррентной языковой модели (hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.**  Выпишите формулы прямого прохода указанной модели: как вычисляется оценка вероятности $\\hat{y_t}[w]$ = $\\hat{P}(w|x_1, . . . , x_{t-1})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала выпишем формулы прямого прохода для первого слоя. (все вектора и матрицы будут с индексом 1).<br>\n",
    "Для разных гейтов разные матрицы весов (верхний индекс)<br>\n",
    "$\\circ$ - поэлементное умножение векторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. $x^{(t)}$ - это индекс токена в словаре, то сначала нужно взять эмбеддинг этого токена как входной вектор.\n",
    "\n",
    "$$e^{(t)} = Emb(x^{(t)})$$\n",
    "\n",
    "И дальше подаём его на вход LSTM ячейки вместе с предыдущими скрытыми состояниями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_1^{(t)} = \\sigma(W_1^{(f)}e^{(t)} + b^{(W)}_{f, 1} + U_1^{(f)}h_1^{(t - 1)} + b^{(U)}_{f, 1})$$\n",
    "\n",
    "$$i_1^{(t)} = \\sigma(W_1^{(i)}e^{(t)} + b^{(W)}_{i, 1} +  U_1^{(i)}h_1^{(t - 1)} + b^{(U)}_{i, 1})$$\n",
    "\n",
    "$$o_1^{(t)} = \\sigma(W_1^{(o)}e^{(t)} + b^{(W)}_{o, 1} + U_1^{(o)}h_1^{(t - 1)} + b^{(U)}_{o, 1})$$\n",
    "\n",
    "$$\\tilde{c}_1^{(t)} = \\tanh(W_1^{(c)}e^{(t)} + b^{(W)}_{c, 1} + U_1^{(c)}h_1^{(t - 1)} + b^{(U)}_{c, 1})$$\n",
    "\n",
    "$$c_1^{(t)} = c_1^{(t - 1)} \\circ f_1^{(t)} + i_1^{(t)} \\circ \\tilde{c}_1^{(t)}$$\n",
    "\n",
    "$$h_1^{(t)} = o_1^{(t)} \\circ \\tanh(c_1^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для второго слоя формулы такие же, отличие в том, что на вход второму слою подаются скрытые состояния из первого слоя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_2^{(t)} = \\sigma(W_2^{(f)}h_1^{(t)} + b^{(W)}_{f, 2} + U_2^{(f)}h_2^{(t - 1)} + b^{(U)}_{f, 2})$$\n",
    "\n",
    "$$i_2^{(t)} = \\sigma(W_2^{(i)}h_1^{(t)} + b^{(W)}_{i, 2} + U_2^{(i)}h_2^{(t - 1)} + b^{(U)}_{i, 2})$$\n",
    "\n",
    "$$o_2^{(t)} = \\sigma(W_2^{(o)}h_1^{(t)} + b^{(W)}_{o, 2} + U_2^{(o)}h_2^{(t - 1)} + b^{(U)}_{o, 2})$$\n",
    "\n",
    "$$\\tilde{c}_2^{(t)} = \\tanh(W_2^{(c)}h_1^{(t)} + b^{(W)}_{c, 2} + U_2^{(c)}h_2^{(t - 1)} + b^{(U)}_{c, 2})$$\n",
    "\n",
    "$$c_2^{(t)} = c_2^{(t - 1)} \\circ f_2^{(t)} + i_2^{(t)} \\circ \\tilde{c}_2^{(t)}$$\n",
    "\n",
    "$$h_2^{(t)} = o_2^{(t)} \\circ \\tanh(c_2^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем, на каждом шаге скрытое состояние второго слоя с помощью линейного преобразования приводится к вектору размерности длины словаря.\n",
    "\n",
    "$$y_t = W^{(L)}h_2^{(t)} + b_3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И дальше этот вектор с помощью софтмакса приводится к вероятностному распределению $\\hat{y_t}[w]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$\\hat{y_t}[w]$ = \\hat{P}(w|x_1, . . . , x_{t-1}) = softmax(y_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "как это записать в одну формулу я не знаю.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.**  Чему равна оценка вероятности последовательности $x_1, x_2, . . . , x_T$ , то есть чему равен $P(x_1, . . . , x_T)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся так называемым **chain rule** для вероятностей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_1, . . . , x_T) = P(x_1) * P(x_2 | x_1) * P(x_3 | x_2 x_1) * ... * P(x_T | x_1x_2...x_{T - 1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И если вводить токены для начала последовательности и для конца последовательности, то оценку вероятности можно записать следующим образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_1, . . . , x_T) = P(x_1|<start>) * P(x_2 | x_1<start>) * P(x_3 | <start>x_1 x_2) * ... * P(x_T | <start>x_1x_2...x_{T - 1}) * P(<eos>|<start>x_1x_2...x_{T - 1}x_T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Оценочная функция для языковой модели (loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.** Покажите, что минимизация кросс-энтропии эквивалентна максимизации правдоподобия правильных текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем функцию правдоподобия для некоторого текста $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(X_i, \\theta) = \\prod_{t=1}^{T_i}\\prod_{j=1}^{V}P(w|x_{i,1},...,x_{i, t-1}, \\theta)^{x_{i, j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь прологарифмируем это всё дело."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$log (L(X_i, \\theta)) = \\sum_{t=1}^{T_i}\\sum_{j=1}^{V}x_{i, j}log P(w|x_{i,1},...,x_{i, t-1}, \\theta)$$ <br>\n",
    "\n",
    "И дальше будем минимизировать $-log (L(X_i, \\theta))$ <br>\n",
    "\n",
    "$$-log (L(X_i, \\theta)) = \\sum_{t=1}^{T_i}\\sum_{j=1}^{V}-x_{i, j}log P(w|x_{i,1},...,x_{i, t-1}, \\theta) \n",
    "= \\sum_{t=1}^{T_i}ce(oh(x_i,t), P(w|x_{i,1}, . . . , x_{i,t−1}, \\theta))$$\n",
    "\n",
    "Итоговый лосс будем брать средний по лоссам для всех токенов в тексте.\n",
    "$$L(X_i, \\theta) = \\frac{1}{T_i}\\sum_{t=1}^{T_i}ce(oh(x_i,t), P(w|x_{i,1}, . . . , x_{i,t−1}, \\theta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Минимизация этого лосса будет максимизировать функцию правдоподобия правильного текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.** Какие специальные токены (не входящие в текст) используют в языковой модели и какую роль они играют при вычислении вероятности текста / генерации нового текста?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В языковой модели используются специальные токены \n",
    "1. $<start>$ сигнализирующий о начале новой последовательности.\n",
    "2. $<eos>$ сигнализирующий о конце текущей последовательности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При генерации нового текста или вычислении вероятности, мы не можем определить вероятностное распределение для первого слова, поэтому нам нужно подать на вход модели некоторый стартовый токен, который позволит нам корректно выбрать первое слово из получаемого распределения.<br>\n",
    "\n",
    "Конечный токен позволяет определять конец(wow) генерируемого предложения. И главное правильно вычислять его вероятность, потому что нельзя просто заканчивать текст на последнем слове, а конечный токен позволяет зафиксировать его длину."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 LSTM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Выпишите формулы прямого прохода для LSTM слоя. Считайте, что на входе слоя – последовательность векторов $e_1, ..., e_T $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного не понятно отличие от вопроса 1.1. Но"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_1^{(t)} = \\sigma(W_1^{(f)}e^{(t)} + b^{(W)}_{f, 1} + U_1^{(f)}h_1^{(t - 1)} + b^{(U)}_{f, 1})$$\n",
    "\n",
    "$$i_1^{(t)} = \\sigma(W_1^{(i)}e^{(t)} + b^{(W)}_{i, 1} +  U_1^{(i)}h_1^{(t - 1)} + b^{(U)}_{i, 1})$$\n",
    "\n",
    "$$o_1^{(t)} = \\sigma(W_1^{(o)}e^{(t)} + b^{(W)}_{o, 1} + U_1^{(o)}h_1^{(t - 1)} + b^{(U)}_{o, 1})$$\n",
    "\n",
    "$$\\tilde{c}_1^{(t)} = \\tanh(W_1^{(c)}e^{(t)} + b^{(W)}_{c, 1} + U_1^{(c)}h_1^{(t - 1)} + b^{(U)}_{c, 1})$$\n",
    "\n",
    "$$c_1^{(t)} = c_1^{(t - 1)} \\circ f_1^{(t)} + i_1^{(t)} \\circ \\tilde{c}_1^{(t)}$$\n",
    "\n",
    "$$h_1^{(t)} = o_1^{(t)} \\circ \\tanh(c_1^{(t)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Объясните, что помогает LSTM бороться с проблемой затухающих градиентов и почему?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема затухающих градиентов в обычных рекуррентных нейронных сетях(и не рекуррентных тоже) заключается в том, что градиент функции потерь по выходам начальных слоёв становится слишком маленьким по модулю, т.к. перемножаются производных следующих выходов по предыдущим. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J^{(t)}}{\\partial h^{(1)}}\n",
    "= \\frac{\\partial h^{(2)}}{\\partial h^{(1)}} * \\frac{\\partial h^{(3)}}{\\partial h^{(2)}} * ... * \\frac{\\partial J^{(t)}}{\\partial h^{(t)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если использовать сигмоиду, как функцию активации, то множители будут ограничены сверху 0.25 и понятно, что 5-7 таких множителей ничего не оставят от градиента.\n",
    "(Если использовать не ограниченные функции активации, то можем столкнуться с проблемой взрыва градиента)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, градиент по последней во времени функции потерь может влиять только на небольшое количество ближайших слоёв. (тут проще представить развернутую во времени рекуррентную нейроную сеть)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В LSTM для борьбы с этим явлением добавить ещё один выход ячейки. <br>\n",
    "Теперь получаем что есть \n",
    "1. $c^{(t)}$ - состояние ячейки. Хранит информацию полученную на протяжении всего обучения.\n",
    "2. $h^{(t)}$ - скрытое состояние. Хранит ближайшую информацию из ближайшей по времени обучения ячейки.\n",
    "\n",
    "Состояние ячейки на каждом шаге обновляется исходя из текущего входа и скрытого состояния на прошлом шаге с помощью 4 гейтов:\n",
    "$$c_1^{(t)} = c_1^{(t - 1)} \\circ f_1^{(t)} + i_1^{(t)} \\circ \\tilde{c}_1^{(t)}$$\n",
    "1. **forget gate** - показывает, какую информацию в состоянии ячейки стоит забыть. Т.к. в процессе обучения веса для  подсчёта этого гейта настраиваются, то сеть может сама обучиться, какую информацию стоит забыть, а какую нет. В отличие от обычных RNN, где вся дальняя информация забывается.\n",
    "\n",
    "2. **input gate** - показывает, какую информацию из новой информации ячейки стоит записать в старую.\n",
    "\n",
    "3. **output gate** - показывает, какую информацию стоит передать в следующую ячейку через скрытое состояние."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После подсчёта гейтов, состояние ячейки обновляет только с помощью поэлементного умножения и сложения, без применения функций активации. Это также позволяет уменьшить затухание градиента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом главное преимущество LSTM заключается в том, что благодаря управлению информацией с помощью гейтов сеть проще пропускает информацию с прошлых слоёв и она не уходит в 0(например, если гейт забывания будет оставлять всю информацию)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Перплексия.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Perplexity(x_1, ... , x_T ) = exp(−\\frac{1}{T}\\sum^{T}_{t=1}log P(x_t|x_1, . . . , x_{t−1}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Выразите пословную перплексию через кросс-энтропию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем сначала кросс-энтропию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(X_i, \\theta) = \\frac{1}{T_i}\\sum_{t=1}^{T_i}ce(oh(x_i,t), P(w|x_{i,1}, . . . , x_{i,t−1}, \\theta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. теперь у нас зафиксированы слова в последовательности (и последовательность одна), то её можно переписать следующим образом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(X, \\theta) = \\frac{1}{T}\\sum_{t=1}^{T}-logP(x_t|x_{1}, . . . , x_{t−1}, \\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}logP(x_t|x_{1}, . . . , x_{t−1}, \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда получаем, что\n",
    "<br>\n",
    "$Perplexity(x_1, ... , x_T ) = exp(L(X, \\theta)) $, где $X = x_1, x_2, ..., x_T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Языковое моделирование для предобучения рекуррентной сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Как вы думаете, какие причины приводят к нестабильному обучению рекуррентных сетей даже с LTSM ячейкой на задаче классификации длинных текстов при случайной начальной инициализации всех весов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я думаю, что обучение происходит нестабильно из-за проблемы угасающих градиентов. Т.к. эмбеддинги токенов инициализируются случайно, то изначально они не несут никакой информации. А из-за того, что тексты длинные, то после некоторого временного шага, эмбеддинги ещё и перестают нормально обучаться.<br>\n",
    "Получаем, что при классификации длинного текста на входы LSTM ячеек мы подаём набор векторов, которые не содержат достаточное количество информации о токенах и поэтому сеть будет искать некоторые непредсказуемые (и зачастую неправильные) закономерности в данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Предположите с чем связано улучшение финального качества классификаторов текстов при предобучении их языковому моделированию?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе предобучения языковому моделированию модель может найти закономерности в языке, и научиться моделировать тональность текста. Примером такого явления можно назвать sentiment neuron, который показывали в работах с предобученными LM-LSTM.\n",
    "Также при предобучении мы обучаем эмбеддинги токенов, которые в этот раз уже лучше отражают зависимости в языке. Это должно помочь модели перестать переобучаться и лучше обобщать последовательности."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
