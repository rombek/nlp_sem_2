{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Практическая часть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "import re\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "from filimdb_evaluation.score import load_dataset_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train set \n",
      "pos 7520\n",
      "neg 7480\n",
      "Loading dev set \n",
      "pos 4980\n",
      "neg 5020\n",
      "Loading test set \n",
      "unlabeled 25000\n"
     ]
    }
   ],
   "source": [
    "all_data = load_dataset_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = all_data['train']\n",
    "dev_dataset = all_data['dev']\n",
    "test_dataset = all_data['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Сделайте предобработку документов по аналогии с предыдущими заданиями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = text.lower()\n",
    "    remove_tags = re.compile(r'<.*?>')\n",
    "    text = re.sub(remove_tags, '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join(sym if (sym.isalnum() or sym in (\" \", \"'\")) else f\" {sym} \" for sym in text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, stem=0):\n",
    "    \"\"\"\n",
    "        arg: list of texts\n",
    "        return: list of tokenized texts\n",
    "    \"\"\"\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', \\\n",
    "    'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with',\n",
    "     'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'could','couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    tokenizer = re.compile(r\"-?\\d*[.,]?\\d+|[?'\\w]+|\\S\", re.MULTILINE | re.IGNORECASE)\n",
    "    tokenized_text = tokenizer.findall(text)\n",
    "    if stem == 0:\n",
    "        return [token for token in tokenized_text if token not in stop_words]\n",
    "    stem_dataset = [token[:stem] for token in tokenized_text if token not in stop_words]\n",
    "    return stem_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    processed_dataset = []\n",
    "    is_test = False\n",
    "    if dataset[2] is None:\n",
    "        dataset = dataset[:2]\n",
    "        is_test = True\n",
    "    for ind, ds_data in enumerate(zip(*dataset)):\n",
    "        prepared_text = preprocessing(ds_data[1])\n",
    "        tokenized_text = tokenize_text(prepared_text)\n",
    "        if not is_test:\n",
    "            processed_dataset.append((tokenized_text, ds_data[2]))\n",
    "        else:\n",
    "            processed_dataset.append(tokenized_text)\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = preprocess_dataset(train_dataset)\n",
    "dev_dataset = preprocess_dataset(dev_dataset)\n",
    "test_dataset = preprocess_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['myth', 'regarding', 'broken', 'mirrors', 'would', 'accurate', 'everybody', 'involved', 'production', 'would', 'face', 'approximately', '170', 'years', 'bad', 'luck', 'lot', 'mirrors', 'falling', 'little', 'pieces', 'script', 'shattering', 'glass', 'broken', 'would', 'brilliant', 'film', 'sadly', 'overlong', 'derivative', 'dull', 'movie', 'handful', 'remarkable', 'ideas', 'memorable', 'sequences', 'sean', 'ellis', 'made', 'stylish', 'elegantly', 'photographed', 'movie', 'story', 'lackluster', 'total', 'absence', 'logic', 'explanation', 'really', 'frustrating', 'got', 'discussion', 'friend', 'regarding', 'basic', 'concept', 'meaning', 'film', 'thinks', 'ellis', 'found', 'inspiration', 'old', 'legend', 'claiming', 'spotting', 'doppelganger', 'foreboding', 'youre', 'going', 'die', 'interesting', 'theory', 'im', 'familiar', 'legend', 'couldnt', 'find', 'anything', 'internet', 'neither', 'personally', 'think', 'broken', 'yet', 'another', 'umpteenth', 'variation', 'theme', 'invasion', 'body', 'snatchers', 'without', 'alien', 'interference', 'broken', 'centers', 'american', 'mcvey', 'family', 'living', 'london', 'particularly', 'gina', 'mirror', 'spontaneously', 'breaks', 'birthday', 'celebration', 'triggers', 'whole', 'series', 'mysterious', 'seemingly', 'supernatural', 'events', 'gina', 'spots', 'driving', 'car', 'follows', 'mirror', 'image', 'apartment', 'building', 'whilst', 'driving', 'home', 'state', 'mental', 'confusion', 'causes', 'terrible', 'car', 'accident', 'ends', 'hospital', 'dismissed', 'gina', 'feels', 'like', 'whole', 'surrounding', 'changing', 'doesnt', 'recognize', 'boyfriend', 'anymore', 'uncanny', 'fragments', 'accident', 'keep', 'flashing', 'eyes', 'suffer', 'mental', 'traumas', 'invoked', 'accident', 'really', 'supernatural', 'conspiracy', 'happening', 'around', 'writerdirector', 'sean', 'ellis', 'definitely', 'invokes', 'feelings', 'curiosity', 'suspense', 'script', 'unfortunately', 'fails', 'properly', 'elaborate', 'broken', 'truly', 'atmospheric', 'stylish', 'effort', 'half', 'hour', 'film', 'come', 'painful', 'conclusion', 'shall', 'remain', 'beautiful', 'empty', 'package', 'theres', 'frustratingly', 'high', 'amount', 'fake', 'suspense', 'film', 'means', 'building', 'tension', 'ominous', 'music', 'eerie', 'camera', 'angels', 'absolutely', 'nothing', 'even', 'happened', 'far', 'time', 'actually', 'mysteriousness', 'kicks', 'tricks', 'dont', 'scary', 'effect', 'anymore', 'fellow', 'reviewers', 'around', 'compare', 'film', 'particularly', 'sean', 'ellis', 'style', 'repertoires', 'david', 'lynch', 'stanley', 'kubrick', 'even', 'alfred', 'hitchcock', 'way', 'way', 'way', 'much', 'honor', 'ps', 'alternate', 'spelling', 'one', 'scandinavian', 'ø'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Замените каждый документ на набор униграмм, биграмм и триграмм, которые в нём встречаются (каждая n-грамма входит в набор столько раз, сколько раз встречается в тексте)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, max_ngram=3):\n",
    "    ngrams = []\n",
    "    for token in text:\n",
    "        ngrams.append(token)\n",
    "    if max_ngram >= 2:\n",
    "        for token in zip(text[:-1], text[1:]):\n",
    "            ngrams.append(token)\n",
    "    if max_ngram >= 3:\n",
    "        for token in zip(text[:-2], text[1:-1], text[2:]):\n",
    "            ngrams.append(token)\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngram_dataset(dataset, max_ngram=3):\n",
    "    ngram_dataset = []\n",
    "    for ind, ds_info in enumerate(dataset):\n",
    "        ngrams = generate_ngrams(ds_info[0])\n",
    "        ngram_dataset.append((ngrams, *ds_info[1:]))\n",
    "    return ngram_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = make_ngram_dataset(train_dataset)\n",
    "dev_dataset = make_ngram_dataset(dev_dataset)\n",
    "test_dataset = make_ngram_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['myth',\n",
       "  'regarding',\n",
       "  'broken',\n",
       "  'mirrors',\n",
       "  'would',\n",
       "  'accurate',\n",
       "  'everybody',\n",
       "  'involved',\n",
       "  'production',\n",
       "  'would',\n",
       "  'face',\n",
       "  'approximately',\n",
       "  '170',\n",
       "  'years',\n",
       "  'bad',\n",
       "  'luck',\n",
       "  'lot',\n",
       "  'mirrors',\n",
       "  'falling',\n",
       "  'little',\n",
       "  'pieces',\n",
       "  'script',\n",
       "  'shattering',\n",
       "  'glass',\n",
       "  'broken',\n",
       "  'would',\n",
       "  'brilliant',\n",
       "  'film',\n",
       "  'sadly',\n",
       "  'overlong',\n",
       "  'derivative',\n",
       "  'dull',\n",
       "  'movie',\n",
       "  'handful',\n",
       "  'remarkable',\n",
       "  'ideas',\n",
       "  'memorable',\n",
       "  'sequences',\n",
       "  'sean',\n",
       "  'ellis',\n",
       "  'made',\n",
       "  'stylish',\n",
       "  'elegantly',\n",
       "  'photographed',\n",
       "  'movie',\n",
       "  'story',\n",
       "  'lackluster',\n",
       "  'total',\n",
       "  'absence',\n",
       "  'logic',\n",
       "  'explanation',\n",
       "  'really',\n",
       "  'frustrating',\n",
       "  'got',\n",
       "  'discussion',\n",
       "  'friend',\n",
       "  'regarding',\n",
       "  'basic',\n",
       "  'concept',\n",
       "  'meaning',\n",
       "  'film',\n",
       "  'thinks',\n",
       "  'ellis',\n",
       "  'found',\n",
       "  'inspiration',\n",
       "  'old',\n",
       "  'legend',\n",
       "  'claiming',\n",
       "  'spotting',\n",
       "  'doppelganger',\n",
       "  'foreboding',\n",
       "  'youre',\n",
       "  'going',\n",
       "  'die',\n",
       "  'interesting',\n",
       "  'theory',\n",
       "  'im',\n",
       "  'familiar',\n",
       "  'legend',\n",
       "  'couldnt',\n",
       "  'find',\n",
       "  'anything',\n",
       "  'internet',\n",
       "  'neither',\n",
       "  'personally',\n",
       "  'think',\n",
       "  'broken',\n",
       "  'yet',\n",
       "  'another',\n",
       "  'umpteenth',\n",
       "  'variation',\n",
       "  'theme',\n",
       "  'invasion',\n",
       "  'body',\n",
       "  'snatchers',\n",
       "  'without',\n",
       "  'alien',\n",
       "  'interference',\n",
       "  'broken',\n",
       "  'centers',\n",
       "  'american',\n",
       "  'mcvey',\n",
       "  'family',\n",
       "  'living',\n",
       "  'london',\n",
       "  'particularly',\n",
       "  'gina',\n",
       "  'mirror',\n",
       "  'spontaneously',\n",
       "  'breaks',\n",
       "  'birthday',\n",
       "  'celebration',\n",
       "  'triggers',\n",
       "  'whole',\n",
       "  'series',\n",
       "  'mysterious',\n",
       "  'seemingly',\n",
       "  'supernatural',\n",
       "  'events',\n",
       "  'gina',\n",
       "  'spots',\n",
       "  'driving',\n",
       "  'car',\n",
       "  'follows',\n",
       "  'mirror',\n",
       "  'image',\n",
       "  'apartment',\n",
       "  'building',\n",
       "  'whilst',\n",
       "  'driving',\n",
       "  'home',\n",
       "  'state',\n",
       "  'mental',\n",
       "  'confusion',\n",
       "  'causes',\n",
       "  'terrible',\n",
       "  'car',\n",
       "  'accident',\n",
       "  'ends',\n",
       "  'hospital',\n",
       "  'dismissed',\n",
       "  'gina',\n",
       "  'feels',\n",
       "  'like',\n",
       "  'whole',\n",
       "  'surrounding',\n",
       "  'changing',\n",
       "  'doesnt',\n",
       "  'recognize',\n",
       "  'boyfriend',\n",
       "  'anymore',\n",
       "  'uncanny',\n",
       "  'fragments',\n",
       "  'accident',\n",
       "  'keep',\n",
       "  'flashing',\n",
       "  'eyes',\n",
       "  'suffer',\n",
       "  'mental',\n",
       "  'traumas',\n",
       "  'invoked',\n",
       "  'accident',\n",
       "  'really',\n",
       "  'supernatural',\n",
       "  'conspiracy',\n",
       "  'happening',\n",
       "  'around',\n",
       "  'writerdirector',\n",
       "  'sean',\n",
       "  'ellis',\n",
       "  'definitely',\n",
       "  'invokes',\n",
       "  'feelings',\n",
       "  'curiosity',\n",
       "  'suspense',\n",
       "  'script',\n",
       "  'unfortunately',\n",
       "  'fails',\n",
       "  'properly',\n",
       "  'elaborate',\n",
       "  'broken',\n",
       "  'truly',\n",
       "  'atmospheric',\n",
       "  'stylish',\n",
       "  'effort',\n",
       "  'half',\n",
       "  'hour',\n",
       "  'film',\n",
       "  'come',\n",
       "  'painful',\n",
       "  'conclusion',\n",
       "  'shall',\n",
       "  'remain',\n",
       "  'beautiful',\n",
       "  'empty',\n",
       "  'package',\n",
       "  'theres',\n",
       "  'frustratingly',\n",
       "  'high',\n",
       "  'amount',\n",
       "  'fake',\n",
       "  'suspense',\n",
       "  'film',\n",
       "  'means',\n",
       "  'building',\n",
       "  'tension',\n",
       "  'ominous',\n",
       "  'music',\n",
       "  'eerie',\n",
       "  'camera',\n",
       "  'angels',\n",
       "  'absolutely',\n",
       "  'nothing',\n",
       "  'even',\n",
       "  'happened',\n",
       "  'far',\n",
       "  'time',\n",
       "  'actually',\n",
       "  'mysteriousness',\n",
       "  'kicks',\n",
       "  'tricks',\n",
       "  'dont',\n",
       "  'scary',\n",
       "  'effect',\n",
       "  'anymore',\n",
       "  'fellow',\n",
       "  'reviewers',\n",
       "  'around',\n",
       "  'compare',\n",
       "  'film',\n",
       "  'particularly',\n",
       "  'sean',\n",
       "  'ellis',\n",
       "  'style',\n",
       "  'repertoires',\n",
       "  'david',\n",
       "  'lynch',\n",
       "  'stanley',\n",
       "  'kubrick',\n",
       "  'even',\n",
       "  'alfred',\n",
       "  'hitchcock',\n",
       "  'way',\n",
       "  'way',\n",
       "  'way',\n",
       "  'much',\n",
       "  'honor',\n",
       "  'ps',\n",
       "  'alternate',\n",
       "  'spelling',\n",
       "  'one',\n",
       "  'scandinavian',\n",
       "  'ø',\n",
       "  ('myth', 'regarding'),\n",
       "  ('regarding', 'broken'),\n",
       "  ('broken', 'mirrors'),\n",
       "  ('mirrors', 'would'),\n",
       "  ('would', 'accurate'),\n",
       "  ('accurate', 'everybody'),\n",
       "  ('everybody', 'involved'),\n",
       "  ('involved', 'production'),\n",
       "  ('production', 'would'),\n",
       "  ('would', 'face'),\n",
       "  ('face', 'approximately'),\n",
       "  ('approximately', '170'),\n",
       "  ('170', 'years'),\n",
       "  ('years', 'bad'),\n",
       "  ('bad', 'luck'),\n",
       "  ('luck', 'lot'),\n",
       "  ('lot', 'mirrors'),\n",
       "  ('mirrors', 'falling'),\n",
       "  ('falling', 'little'),\n",
       "  ('little', 'pieces'),\n",
       "  ('pieces', 'script'),\n",
       "  ('script', 'shattering'),\n",
       "  ('shattering', 'glass'),\n",
       "  ('glass', 'broken'),\n",
       "  ('broken', 'would'),\n",
       "  ('would', 'brilliant'),\n",
       "  ('brilliant', 'film'),\n",
       "  ('film', 'sadly'),\n",
       "  ('sadly', 'overlong'),\n",
       "  ('overlong', 'derivative'),\n",
       "  ('derivative', 'dull'),\n",
       "  ('dull', 'movie'),\n",
       "  ('movie', 'handful'),\n",
       "  ('handful', 'remarkable'),\n",
       "  ('remarkable', 'ideas'),\n",
       "  ('ideas', 'memorable'),\n",
       "  ('memorable', 'sequences'),\n",
       "  ('sequences', 'sean'),\n",
       "  ('sean', 'ellis'),\n",
       "  ('ellis', 'made'),\n",
       "  ('made', 'stylish'),\n",
       "  ('stylish', 'elegantly'),\n",
       "  ('elegantly', 'photographed'),\n",
       "  ('photographed', 'movie'),\n",
       "  ('movie', 'story'),\n",
       "  ('story', 'lackluster'),\n",
       "  ('lackluster', 'total'),\n",
       "  ('total', 'absence'),\n",
       "  ('absence', 'logic'),\n",
       "  ('logic', 'explanation'),\n",
       "  ('explanation', 'really'),\n",
       "  ('really', 'frustrating'),\n",
       "  ('frustrating', 'got'),\n",
       "  ('got', 'discussion'),\n",
       "  ('discussion', 'friend'),\n",
       "  ('friend', 'regarding'),\n",
       "  ('regarding', 'basic'),\n",
       "  ('basic', 'concept'),\n",
       "  ('concept', 'meaning'),\n",
       "  ('meaning', 'film'),\n",
       "  ('film', 'thinks'),\n",
       "  ('thinks', 'ellis'),\n",
       "  ('ellis', 'found'),\n",
       "  ('found', 'inspiration'),\n",
       "  ('inspiration', 'old'),\n",
       "  ('old', 'legend'),\n",
       "  ('legend', 'claiming'),\n",
       "  ('claiming', 'spotting'),\n",
       "  ('spotting', 'doppelganger'),\n",
       "  ('doppelganger', 'foreboding'),\n",
       "  ('foreboding', 'youre'),\n",
       "  ('youre', 'going'),\n",
       "  ('going', 'die'),\n",
       "  ('die', 'interesting'),\n",
       "  ('interesting', 'theory'),\n",
       "  ('theory', 'im'),\n",
       "  ('im', 'familiar'),\n",
       "  ('familiar', 'legend'),\n",
       "  ('legend', 'couldnt'),\n",
       "  ('couldnt', 'find'),\n",
       "  ('find', 'anything'),\n",
       "  ('anything', 'internet'),\n",
       "  ('internet', 'neither'),\n",
       "  ('neither', 'personally'),\n",
       "  ('personally', 'think'),\n",
       "  ('think', 'broken'),\n",
       "  ('broken', 'yet'),\n",
       "  ('yet', 'another'),\n",
       "  ('another', 'umpteenth'),\n",
       "  ('umpteenth', 'variation'),\n",
       "  ('variation', 'theme'),\n",
       "  ('theme', 'invasion'),\n",
       "  ('invasion', 'body'),\n",
       "  ('body', 'snatchers'),\n",
       "  ('snatchers', 'without'),\n",
       "  ('without', 'alien'),\n",
       "  ('alien', 'interference'),\n",
       "  ('interference', 'broken'),\n",
       "  ('broken', 'centers'),\n",
       "  ('centers', 'american'),\n",
       "  ('american', 'mcvey'),\n",
       "  ('mcvey', 'family'),\n",
       "  ('family', 'living'),\n",
       "  ('living', 'london'),\n",
       "  ('london', 'particularly'),\n",
       "  ('particularly', 'gina'),\n",
       "  ('gina', 'mirror'),\n",
       "  ('mirror', 'spontaneously'),\n",
       "  ('spontaneously', 'breaks'),\n",
       "  ('breaks', 'birthday'),\n",
       "  ('birthday', 'celebration'),\n",
       "  ('celebration', 'triggers'),\n",
       "  ('triggers', 'whole'),\n",
       "  ('whole', 'series'),\n",
       "  ('series', 'mysterious'),\n",
       "  ('mysterious', 'seemingly'),\n",
       "  ('seemingly', 'supernatural'),\n",
       "  ('supernatural', 'events'),\n",
       "  ('events', 'gina'),\n",
       "  ('gina', 'spots'),\n",
       "  ('spots', 'driving'),\n",
       "  ('driving', 'car'),\n",
       "  ('car', 'follows'),\n",
       "  ('follows', 'mirror'),\n",
       "  ('mirror', 'image'),\n",
       "  ('image', 'apartment'),\n",
       "  ('apartment', 'building'),\n",
       "  ('building', 'whilst'),\n",
       "  ('whilst', 'driving'),\n",
       "  ('driving', 'home'),\n",
       "  ('home', 'state'),\n",
       "  ('state', 'mental'),\n",
       "  ('mental', 'confusion'),\n",
       "  ('confusion', 'causes'),\n",
       "  ('causes', 'terrible'),\n",
       "  ('terrible', 'car'),\n",
       "  ('car', 'accident'),\n",
       "  ('accident', 'ends'),\n",
       "  ('ends', 'hospital'),\n",
       "  ('hospital', 'dismissed'),\n",
       "  ('dismissed', 'gina'),\n",
       "  ('gina', 'feels'),\n",
       "  ('feels', 'like'),\n",
       "  ('like', 'whole'),\n",
       "  ('whole', 'surrounding'),\n",
       "  ('surrounding', 'changing'),\n",
       "  ('changing', 'doesnt'),\n",
       "  ('doesnt', 'recognize'),\n",
       "  ('recognize', 'boyfriend'),\n",
       "  ('boyfriend', 'anymore'),\n",
       "  ('anymore', 'uncanny'),\n",
       "  ('uncanny', 'fragments'),\n",
       "  ('fragments', 'accident'),\n",
       "  ('accident', 'keep'),\n",
       "  ('keep', 'flashing'),\n",
       "  ('flashing', 'eyes'),\n",
       "  ('eyes', 'suffer'),\n",
       "  ('suffer', 'mental'),\n",
       "  ('mental', 'traumas'),\n",
       "  ('traumas', 'invoked'),\n",
       "  ('invoked', 'accident'),\n",
       "  ('accident', 'really'),\n",
       "  ('really', 'supernatural'),\n",
       "  ('supernatural', 'conspiracy'),\n",
       "  ('conspiracy', 'happening'),\n",
       "  ('happening', 'around'),\n",
       "  ('around', 'writerdirector'),\n",
       "  ('writerdirector', 'sean'),\n",
       "  ('sean', 'ellis'),\n",
       "  ('ellis', 'definitely'),\n",
       "  ('definitely', 'invokes'),\n",
       "  ('invokes', 'feelings'),\n",
       "  ('feelings', 'curiosity'),\n",
       "  ('curiosity', 'suspense'),\n",
       "  ('suspense', 'script'),\n",
       "  ('script', 'unfortunately'),\n",
       "  ('unfortunately', 'fails'),\n",
       "  ('fails', 'properly'),\n",
       "  ('properly', 'elaborate'),\n",
       "  ('elaborate', 'broken'),\n",
       "  ('broken', 'truly'),\n",
       "  ('truly', 'atmospheric'),\n",
       "  ('atmospheric', 'stylish'),\n",
       "  ('stylish', 'effort'),\n",
       "  ('effort', 'half'),\n",
       "  ('half', 'hour'),\n",
       "  ('hour', 'film'),\n",
       "  ('film', 'come'),\n",
       "  ('come', 'painful'),\n",
       "  ('painful', 'conclusion'),\n",
       "  ('conclusion', 'shall'),\n",
       "  ('shall', 'remain'),\n",
       "  ('remain', 'beautiful'),\n",
       "  ('beautiful', 'empty'),\n",
       "  ('empty', 'package'),\n",
       "  ('package', 'theres'),\n",
       "  ('theres', 'frustratingly'),\n",
       "  ('frustratingly', 'high'),\n",
       "  ('high', 'amount'),\n",
       "  ('amount', 'fake'),\n",
       "  ('fake', 'suspense'),\n",
       "  ('suspense', 'film'),\n",
       "  ('film', 'means'),\n",
       "  ('means', 'building'),\n",
       "  ('building', 'tension'),\n",
       "  ('tension', 'ominous'),\n",
       "  ('ominous', 'music'),\n",
       "  ('music', 'eerie'),\n",
       "  ('eerie', 'camera'),\n",
       "  ('camera', 'angels'),\n",
       "  ('angels', 'absolutely'),\n",
       "  ('absolutely', 'nothing'),\n",
       "  ('nothing', 'even'),\n",
       "  ('even', 'happened'),\n",
       "  ('happened', 'far'),\n",
       "  ('far', 'time'),\n",
       "  ('time', 'actually'),\n",
       "  ('actually', 'mysteriousness'),\n",
       "  ('mysteriousness', 'kicks'),\n",
       "  ('kicks', 'tricks'),\n",
       "  ('tricks', 'dont'),\n",
       "  ('dont', 'scary'),\n",
       "  ('scary', 'effect'),\n",
       "  ('effect', 'anymore'),\n",
       "  ('anymore', 'fellow'),\n",
       "  ('fellow', 'reviewers'),\n",
       "  ('reviewers', 'around'),\n",
       "  ('around', 'compare'),\n",
       "  ('compare', 'film'),\n",
       "  ('film', 'particularly'),\n",
       "  ('particularly', 'sean'),\n",
       "  ('sean', 'ellis'),\n",
       "  ('ellis', 'style'),\n",
       "  ('style', 'repertoires'),\n",
       "  ('repertoires', 'david'),\n",
       "  ('david', 'lynch'),\n",
       "  ('lynch', 'stanley'),\n",
       "  ('stanley', 'kubrick'),\n",
       "  ('kubrick', 'even'),\n",
       "  ('even', 'alfred'),\n",
       "  ('alfred', 'hitchcock'),\n",
       "  ('hitchcock', 'way'),\n",
       "  ('way', 'way'),\n",
       "  ('way', 'way'),\n",
       "  ('way', 'much'),\n",
       "  ('much', 'honor'),\n",
       "  ('honor', 'ps'),\n",
       "  ('ps', 'alternate'),\n",
       "  ('alternate', 'spelling'),\n",
       "  ('spelling', 'one'),\n",
       "  ('one', 'scandinavian'),\n",
       "  ('scandinavian', 'ø'),\n",
       "  ('myth', 'regarding', 'broken'),\n",
       "  ('regarding', 'broken', 'mirrors'),\n",
       "  ('broken', 'mirrors', 'would'),\n",
       "  ('mirrors', 'would', 'accurate'),\n",
       "  ('would', 'accurate', 'everybody'),\n",
       "  ('accurate', 'everybody', 'involved'),\n",
       "  ('everybody', 'involved', 'production'),\n",
       "  ('involved', 'production', 'would'),\n",
       "  ('production', 'would', 'face'),\n",
       "  ('would', 'face', 'approximately'),\n",
       "  ('face', 'approximately', '170'),\n",
       "  ('approximately', '170', 'years'),\n",
       "  ('170', 'years', 'bad'),\n",
       "  ('years', 'bad', 'luck'),\n",
       "  ('bad', 'luck', 'lot'),\n",
       "  ('luck', 'lot', 'mirrors'),\n",
       "  ('lot', 'mirrors', 'falling'),\n",
       "  ('mirrors', 'falling', 'little'),\n",
       "  ('falling', 'little', 'pieces'),\n",
       "  ('little', 'pieces', 'script'),\n",
       "  ('pieces', 'script', 'shattering'),\n",
       "  ('script', 'shattering', 'glass'),\n",
       "  ('shattering', 'glass', 'broken'),\n",
       "  ('glass', 'broken', 'would'),\n",
       "  ('broken', 'would', 'brilliant'),\n",
       "  ('would', 'brilliant', 'film'),\n",
       "  ('brilliant', 'film', 'sadly'),\n",
       "  ('film', 'sadly', 'overlong'),\n",
       "  ('sadly', 'overlong', 'derivative'),\n",
       "  ('overlong', 'derivative', 'dull'),\n",
       "  ('derivative', 'dull', 'movie'),\n",
       "  ('dull', 'movie', 'handful'),\n",
       "  ('movie', 'handful', 'remarkable'),\n",
       "  ('handful', 'remarkable', 'ideas'),\n",
       "  ('remarkable', 'ideas', 'memorable'),\n",
       "  ('ideas', 'memorable', 'sequences'),\n",
       "  ('memorable', 'sequences', 'sean'),\n",
       "  ('sequences', 'sean', 'ellis'),\n",
       "  ('sean', 'ellis', 'made'),\n",
       "  ('ellis', 'made', 'stylish'),\n",
       "  ('made', 'stylish', 'elegantly'),\n",
       "  ('stylish', 'elegantly', 'photographed'),\n",
       "  ('elegantly', 'photographed', 'movie'),\n",
       "  ('photographed', 'movie', 'story'),\n",
       "  ('movie', 'story', 'lackluster'),\n",
       "  ('story', 'lackluster', 'total'),\n",
       "  ('lackluster', 'total', 'absence'),\n",
       "  ('total', 'absence', 'logic'),\n",
       "  ('absence', 'logic', 'explanation'),\n",
       "  ('logic', 'explanation', 'really'),\n",
       "  ('explanation', 'really', 'frustrating'),\n",
       "  ('really', 'frustrating', 'got'),\n",
       "  ('frustrating', 'got', 'discussion'),\n",
       "  ('got', 'discussion', 'friend'),\n",
       "  ('discussion', 'friend', 'regarding'),\n",
       "  ('friend', 'regarding', 'basic'),\n",
       "  ('regarding', 'basic', 'concept'),\n",
       "  ('basic', 'concept', 'meaning'),\n",
       "  ('concept', 'meaning', 'film'),\n",
       "  ('meaning', 'film', 'thinks'),\n",
       "  ('film', 'thinks', 'ellis'),\n",
       "  ('thinks', 'ellis', 'found'),\n",
       "  ('ellis', 'found', 'inspiration'),\n",
       "  ('found', 'inspiration', 'old'),\n",
       "  ('inspiration', 'old', 'legend'),\n",
       "  ('old', 'legend', 'claiming'),\n",
       "  ('legend', 'claiming', 'spotting'),\n",
       "  ('claiming', 'spotting', 'doppelganger'),\n",
       "  ('spotting', 'doppelganger', 'foreboding'),\n",
       "  ('doppelganger', 'foreboding', 'youre'),\n",
       "  ('foreboding', 'youre', 'going'),\n",
       "  ('youre', 'going', 'die'),\n",
       "  ('going', 'die', 'interesting'),\n",
       "  ('die', 'interesting', 'theory'),\n",
       "  ('interesting', 'theory', 'im'),\n",
       "  ('theory', 'im', 'familiar'),\n",
       "  ('im', 'familiar', 'legend'),\n",
       "  ('familiar', 'legend', 'couldnt'),\n",
       "  ('legend', 'couldnt', 'find'),\n",
       "  ('couldnt', 'find', 'anything'),\n",
       "  ('find', 'anything', 'internet'),\n",
       "  ('anything', 'internet', 'neither'),\n",
       "  ('internet', 'neither', 'personally'),\n",
       "  ('neither', 'personally', 'think'),\n",
       "  ('personally', 'think', 'broken'),\n",
       "  ('think', 'broken', 'yet'),\n",
       "  ('broken', 'yet', 'another'),\n",
       "  ('yet', 'another', 'umpteenth'),\n",
       "  ('another', 'umpteenth', 'variation'),\n",
       "  ('umpteenth', 'variation', 'theme'),\n",
       "  ('variation', 'theme', 'invasion'),\n",
       "  ('theme', 'invasion', 'body'),\n",
       "  ('invasion', 'body', 'snatchers'),\n",
       "  ('body', 'snatchers', 'without'),\n",
       "  ('snatchers', 'without', 'alien'),\n",
       "  ('without', 'alien', 'interference'),\n",
       "  ('alien', 'interference', 'broken'),\n",
       "  ('interference', 'broken', 'centers'),\n",
       "  ('broken', 'centers', 'american'),\n",
       "  ('centers', 'american', 'mcvey'),\n",
       "  ('american', 'mcvey', 'family'),\n",
       "  ('mcvey', 'family', 'living'),\n",
       "  ('family', 'living', 'london'),\n",
       "  ('living', 'london', 'particularly'),\n",
       "  ('london', 'particularly', 'gina'),\n",
       "  ('particularly', 'gina', 'mirror'),\n",
       "  ('gina', 'mirror', 'spontaneously'),\n",
       "  ('mirror', 'spontaneously', 'breaks'),\n",
       "  ('spontaneously', 'breaks', 'birthday'),\n",
       "  ('breaks', 'birthday', 'celebration'),\n",
       "  ('birthday', 'celebration', 'triggers'),\n",
       "  ('celebration', 'triggers', 'whole'),\n",
       "  ('triggers', 'whole', 'series'),\n",
       "  ('whole', 'series', 'mysterious'),\n",
       "  ('series', 'mysterious', 'seemingly'),\n",
       "  ('mysterious', 'seemingly', 'supernatural'),\n",
       "  ('seemingly', 'supernatural', 'events'),\n",
       "  ('supernatural', 'events', 'gina'),\n",
       "  ('events', 'gina', 'spots'),\n",
       "  ('gina', 'spots', 'driving'),\n",
       "  ('spots', 'driving', 'car'),\n",
       "  ('driving', 'car', 'follows'),\n",
       "  ('car', 'follows', 'mirror'),\n",
       "  ('follows', 'mirror', 'image'),\n",
       "  ('mirror', 'image', 'apartment'),\n",
       "  ('image', 'apartment', 'building'),\n",
       "  ('apartment', 'building', 'whilst'),\n",
       "  ('building', 'whilst', 'driving'),\n",
       "  ('whilst', 'driving', 'home'),\n",
       "  ('driving', 'home', 'state'),\n",
       "  ('home', 'state', 'mental'),\n",
       "  ('state', 'mental', 'confusion'),\n",
       "  ('mental', 'confusion', 'causes'),\n",
       "  ('confusion', 'causes', 'terrible'),\n",
       "  ('causes', 'terrible', 'car'),\n",
       "  ('terrible', 'car', 'accident'),\n",
       "  ('car', 'accident', 'ends'),\n",
       "  ('accident', 'ends', 'hospital'),\n",
       "  ('ends', 'hospital', 'dismissed'),\n",
       "  ('hospital', 'dismissed', 'gina'),\n",
       "  ('dismissed', 'gina', 'feels'),\n",
       "  ('gina', 'feels', 'like'),\n",
       "  ('feels', 'like', 'whole'),\n",
       "  ('like', 'whole', 'surrounding'),\n",
       "  ('whole', 'surrounding', 'changing'),\n",
       "  ('surrounding', 'changing', 'doesnt'),\n",
       "  ('changing', 'doesnt', 'recognize'),\n",
       "  ('doesnt', 'recognize', 'boyfriend'),\n",
       "  ('recognize', 'boyfriend', 'anymore'),\n",
       "  ('boyfriend', 'anymore', 'uncanny'),\n",
       "  ('anymore', 'uncanny', 'fragments'),\n",
       "  ('uncanny', 'fragments', 'accident'),\n",
       "  ('fragments', 'accident', 'keep'),\n",
       "  ('accident', 'keep', 'flashing'),\n",
       "  ('keep', 'flashing', 'eyes'),\n",
       "  ('flashing', 'eyes', 'suffer'),\n",
       "  ('eyes', 'suffer', 'mental'),\n",
       "  ('suffer', 'mental', 'traumas'),\n",
       "  ('mental', 'traumas', 'invoked'),\n",
       "  ('traumas', 'invoked', 'accident'),\n",
       "  ('invoked', 'accident', 'really'),\n",
       "  ('accident', 'really', 'supernatural'),\n",
       "  ('really', 'supernatural', 'conspiracy'),\n",
       "  ('supernatural', 'conspiracy', 'happening'),\n",
       "  ('conspiracy', 'happening', 'around'),\n",
       "  ('happening', 'around', 'writerdirector'),\n",
       "  ('around', 'writerdirector', 'sean'),\n",
       "  ('writerdirector', 'sean', 'ellis'),\n",
       "  ('sean', 'ellis', 'definitely'),\n",
       "  ('ellis', 'definitely', 'invokes'),\n",
       "  ('definitely', 'invokes', 'feelings'),\n",
       "  ('invokes', 'feelings', 'curiosity'),\n",
       "  ('feelings', 'curiosity', 'suspense'),\n",
       "  ('curiosity', 'suspense', 'script'),\n",
       "  ('suspense', 'script', 'unfortunately'),\n",
       "  ('script', 'unfortunately', 'fails'),\n",
       "  ('unfortunately', 'fails', 'properly'),\n",
       "  ('fails', 'properly', 'elaborate'),\n",
       "  ('properly', 'elaborate', 'broken'),\n",
       "  ('elaborate', 'broken', 'truly'),\n",
       "  ('broken', 'truly', 'atmospheric'),\n",
       "  ('truly', 'atmospheric', 'stylish'),\n",
       "  ('atmospheric', 'stylish', 'effort'),\n",
       "  ('stylish', 'effort', 'half'),\n",
       "  ('effort', 'half', 'hour'),\n",
       "  ('half', 'hour', 'film'),\n",
       "  ('hour', 'film', 'come'),\n",
       "  ('film', 'come', 'painful'),\n",
       "  ('come', 'painful', 'conclusion'),\n",
       "  ('painful', 'conclusion', 'shall'),\n",
       "  ('conclusion', 'shall', 'remain'),\n",
       "  ('shall', 'remain', 'beautiful'),\n",
       "  ('remain', 'beautiful', 'empty'),\n",
       "  ('beautiful', 'empty', 'package'),\n",
       "  ('empty', 'package', 'theres'),\n",
       "  ('package', 'theres', 'frustratingly'),\n",
       "  ('theres', 'frustratingly', 'high'),\n",
       "  ('frustratingly', 'high', 'amount'),\n",
       "  ('high', 'amount', 'fake'),\n",
       "  ('amount', 'fake', 'suspense'),\n",
       "  ('fake', 'suspense', 'film'),\n",
       "  ('suspense', 'film', 'means'),\n",
       "  ('film', 'means', 'building'),\n",
       "  ('means', 'building', 'tension'),\n",
       "  ('building', 'tension', 'ominous'),\n",
       "  ('tension', 'ominous', 'music'),\n",
       "  ('ominous', 'music', 'eerie'),\n",
       "  ('music', 'eerie', 'camera'),\n",
       "  ('eerie', 'camera', 'angels'),\n",
       "  ('camera', 'angels', 'absolutely'),\n",
       "  ('angels', 'absolutely', 'nothing'),\n",
       "  ('absolutely', 'nothing', 'even'),\n",
       "  ('nothing', 'even', 'happened'),\n",
       "  ('even', 'happened', 'far'),\n",
       "  ('happened', 'far', 'time'),\n",
       "  ('far', 'time', 'actually'),\n",
       "  ('time', 'actually', 'mysteriousness'),\n",
       "  ('actually', 'mysteriousness', 'kicks'),\n",
       "  ('mysteriousness', 'kicks', 'tricks'),\n",
       "  ('kicks', 'tricks', 'dont'),\n",
       "  ('tricks', 'dont', 'scary'),\n",
       "  ('dont', 'scary', 'effect'),\n",
       "  ('scary', 'effect', 'anymore'),\n",
       "  ('effect', 'anymore', 'fellow'),\n",
       "  ('anymore', 'fellow', 'reviewers'),\n",
       "  ('fellow', 'reviewers', 'around'),\n",
       "  ('reviewers', 'around', 'compare'),\n",
       "  ('around', 'compare', 'film'),\n",
       "  ('compare', 'film', 'particularly'),\n",
       "  ('film', 'particularly', 'sean'),\n",
       "  ('particularly', 'sean', 'ellis'),\n",
       "  ('sean', 'ellis', 'style'),\n",
       "  ('ellis', 'style', 'repertoires'),\n",
       "  ('style', 'repertoires', 'david'),\n",
       "  ('repertoires', 'david', 'lynch'),\n",
       "  ('david', 'lynch', 'stanley'),\n",
       "  ('lynch', 'stanley', 'kubrick'),\n",
       "  ('stanley', 'kubrick', 'even'),\n",
       "  ('kubrick', 'even', 'alfred'),\n",
       "  ('even', 'alfred', 'hitchcock'),\n",
       "  ('alfred', 'hitchcock', 'way'),\n",
       "  ('hitchcock', 'way', 'way'),\n",
       "  ('way', 'way', 'way'),\n",
       "  ('way', 'way', 'much'),\n",
       "  ('way', 'much', 'honor'),\n",
       "  ('much', 'honor', 'ps'),\n",
       "  ('honor', 'ps', 'alternate'),\n",
       "  ('ps', 'alternate', 'spelling'),\n",
       "  ('alternate', 'spelling', 'one'),\n",
       "  ('spelling', 'one', 'scandinavian'),\n",
       "  ('one', 'scandinavian', 'ø')],\n",
       " 'neg')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Создайте словарь из индексов n-грамм и посчитайте кол-во вхождений в корпус каждой n-граммы. Сделайте отсечку по частоте встречаемости в корпусе, так чтобы осталось около миллиона элементов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(datasets, max_df = 0.5, min_df = 10, min_tf = 3, max_tokens = 1000000):\n",
    "    df_cnt = defaultdict(int)\n",
    "    tf_cnt = defaultdict(int)\n",
    "    total_documents = sum(len(ds) for ds in datasets)\n",
    "    for ds in datasets:\n",
    "        for ds_info in ds:\n",
    "            been = set()\n",
    "            for token in ds_info[0]:\n",
    "                if  token not in been:\n",
    "                    been.add(token)\n",
    "                    df_cnt[token] += 1\n",
    "                tf_cnt[token] += 1\n",
    "                \n",
    "    free_ind = 0\n",
    "    w2ind = dict()\n",
    "    vocab_tf = []\n",
    "    tf_with_inds = []\n",
    "    for word, tf in tf_cnt.items():\n",
    "        df = df_cnt[word]\n",
    "        if tf >= min_tf and df / total_documents <= max_df and df >= min_df:\n",
    "            w2ind[word] = free_ind\n",
    "            vocab_tf.append(tf)\n",
    "            tf_with_inds.append((tf, word))\n",
    "            free_ind += 1\n",
    "            \n",
    "    tf_with_inds.sort(key=lambda x:x[0], reverse=True)\n",
    "    for tf, w in tf_with_inds[max_tokens:]:\n",
    "        del w2ind[w]\n",
    "    \n",
    "    free_ind = 0\n",
    "    w2ind_final = dict()\n",
    "    vocab_tf_final = []\n",
    "    for w, ind in w2ind.items():\n",
    "        w2ind_final[w] = free_ind\n",
    "        vocab_tf_final.append(tf_cnt[w])\n",
    "        free_ind += 1\n",
    "        \n",
    "    return w2ind_final, vocab_tf_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2ind_full, vocab_tf_full = make_vocab([train_dataset, dev_dataset, test_dataset], min_tf=1, max_df=1.0, min_df=1, max_tokens=5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4851187\n"
     ]
    }
   ],
   "source": [
    "print(len(w2ind_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим гистограмму встречаемости слов в словаре без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(vocab_tf_full, log=True, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь не очень видно, но очень много токенов встречается в корпусе 1 раз.\n",
    "<br>\n",
    "Такое очень сложно как-то ограничить частотами до миллиона, поэтому просто отсортируем все слова по частоте встречаемости и оставим с наибольшими частотами миллион токенов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2ind, vocab_tf = make_vocab([train_dataset, dev_dataset, test_dataset], min_tf=1, max_df=0.7, min_df=1, max_tokens=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    }
   ],
   "source": [
    "print(len(w2ind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нормас."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Возведите количество вхождений в корпусе каждой n-граммы в степень 3/4. А затем нормализуйте так, чтобы получились вероятности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_tf = np.array(vocab_tf, dtype=np.float64)\n",
    "vocab_tf_prob = np.float_power(vocab_tf, 0.75)\n",
    "vocab_tf_prob /= vocab_tf_prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_tf_prob.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Все документы представьте в виде двух массивов данных одинакового размера: в первый массив сохраните все индексы n-грамм для всех документов, во второй массив — индексы документов для соответствующих n-грамм в первом массиве. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Склеим все датасеты в один для обучения эмбеддингов документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "transductive_dataset = list(chain(train_dataset, dev_dataset, test_dataset))\n",
    "print(len(transductive_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 10000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset), len(dev_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "transductive_texts = [x[0] for x in transductive_dataset]\n",
    "print(len(transductive_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inds_ngram_dataset(texts, w2ind, shuffle=True):\n",
    "    ngrams_inds = []\n",
    "    docs_inds = []\n",
    "    for doc_ind, text in enumerate(texts):\n",
    "        for ngram in text:\n",
    "            if ngram in w2ind:\n",
    "                ngrams_inds.append(w2ind[ngram])\n",
    "                docs_inds.append(doc_ind)\n",
    "            \n",
    "    ngrams_inds = np.array(ngrams_inds)\n",
    "    docs_inds = np.array(docs_inds)\n",
    "    assert(len(ngrams_inds) == len(docs_inds))\n",
    "    if shuffle:\n",
    "        permutation = np.random.permutation(len(docs_inds))\n",
    "        ngrams_inds = ngrams_inds[permutation]\n",
    "        docs_inds = docs_inds[permutation]\n",
    "    return ngrams_inds, docs_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transductive_inds_dataset = make_inds_ngram_dataset(transductive_texts, w2ind, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5494618\n",
      "5494618\n"
     ]
    }
   ],
   "source": [
    "print(len(transductive_inds_dataset[0]))\n",
    "print(len(transductive_inds_dataset[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Напишите функцию batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(words_idxs, docs_idxs, probs, nb=5, batch_size=100):\n",
    "    # Let's generate all negative examples at once.\n",
    "    \n",
    "    neg_samples = np.random.choice(np.arange(len(probs)), size = (nb * len(words_idxs), ), p=probs)\n",
    "    \n",
    "#     print(\"pos_samples_len = \", len(docs_idxs), \", neg_samples = \", len(neg_samples))\n",
    "    \n",
    "    end = (len(words_idxs) // batch_size - 1) * batch_size + 1\n",
    "    for batch_start in range(0, end, batch_size):\n",
    "        pos_batch = words_idxs[batch_start : batch_start + batch_size]\n",
    "        docs_batch = docs_idxs[batch_start : batch_start + batch_size]\n",
    "        pos_labels_batch = np.array([1 for _ in range(len(pos_batch))])\n",
    "        yield (pos_batch, docs_batch, pos_labels_batch)\n",
    "        for i in range(nb):\n",
    "            neg_batch = neg_samples[batch_start + batch_size * i : batch_start + batch_size * (i + 1)]\n",
    "            neg_labels_batch = np.array([0 for _ in range(len(neg_batch))])\n",
    "            yield (neg_batch, docs_batch, neg_labels_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bg = batch_generator(transductive_inds_dataset[0], transductive_inds_dataset[1], vocab_tf_prob, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([444905, 318999,  50459, 195750,    722]),\n",
       " array([13451, 22374,   178,   750, 13147]),\n",
       " array([1, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(test_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([932707,  68086,  74010,   1352,  37643]),\n",
       " array([13451, 22374,   178,   750, 13147]),\n",
       " array([0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(test_bg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Далее создадим класс Doc2Vec,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2Vec:\n",
    "    def __init__(self, vocab_size, docs_cnt, emb_size=500, train_start = 0):\n",
    "        self.word_embs = np.random.uniform(low=-0.001, high=0.001, size=(vocab_size, emb_size))\n",
    "        self.docs_embs = np.random.uniform(low=-0.001, high=0.001, size=(docs_cnt, emb_size))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.docs_cnt = docs_cnt\n",
    "        self.emb_size = emb_size\n",
    "        self.train_start = train_start\n",
    "    \n",
    "    def train(self, word_inds, doc_inds, labels, lr):\n",
    "        word_batch_embs = self.word_embs[word_inds]\n",
    "        doc_batch_embs = self.docs_embs[doc_inds]\n",
    "        \n",
    "        dot_prods = np.einsum('ij,ij->i', word_batch_embs, doc_batch_embs)\n",
    "        y_pred = self.sigmoid(dot_prods)\n",
    "        \n",
    "        word_batch_grads = doc_batch_embs * (y_pred - labels).reshape(-1, 1)\n",
    "        doc_batch_grads = word_batch_embs * (y_pred - labels).reshape(-1, 1)\n",
    "        \n",
    "        for ind, (w_ind, d_ind) in enumerate(zip(word_inds, doc_inds)):\n",
    "            self.word_embs[w_ind] -= lr * word_batch_grads[ind]\n",
    "            self.docs_embs[d_ind] -= lr * doc_batch_grads[ind]\n",
    "    \n",
    "        batch_loss = (-labels * np.log(y_pred) - (1 - labels) * np.log(1 - y_pred)).sum()\n",
    "        \n",
    "        return batch_loss\n",
    "            \n",
    "    def get_all_X(self):\n",
    "        train_borders = (self.train_start, self.train_start + 15000)\n",
    "        dev_borders = (train_borders[1], train_borders[1] + 10000)\n",
    "        test_borders = (dev_borders[1], dev_borders[1] + 25000)\n",
    "        X_train = self.docs_embs[train_borders[0] : train_borders[1]]\n",
    "        X_dev = self.docs_embs[dev_borders[0] : dev_borders[1]]\n",
    "        X_test = self.docs_embs[test_borders[0] : test_borders[1]]\n",
    "    \n",
    "        return X_train, X_dev, X_test\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return np.where(x > 0, 1.0 / (1.0 + np.exp(-x)), np.exp(x) / (np.exp(x) + 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Напишите функцию, которая будет принимать эмбединги и метки документов из train и dev выборок, обучать логистическую регрессию на обучающей и возвращать ее точность на обучающей и валидационной выборках."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg(X_train, y_train, X_dev, y_dev):\n",
    "    model = LogisticRegression(penalty='l2', max_iter=500)\n",
    "    \n",
    "    log_border = 3\n",
    "    C_values = np.logspace(-log_border, log_border, 30)\n",
    "    params = {'C' : C_values}\n",
    "    gs_clf = GridSearchCV(model, params, cv=10, n_jobs=4, verbose=1)\n",
    "    gs_clf.fit(X_train, y_train)\n",
    "    \n",
    "    if gs_clf.best_params_['C'] in (C_values[0], C_values[-1]):\n",
    "        print(\"C is on border!\")\n",
    "        log_border += 2\n",
    "        C_values = np.logspace(-log_border, log_border, 30)\n",
    "        params = {'C' : C_values}\n",
    "        gs_clf = GridSearchCV(model, params, cv=10, n_jobs=4, verbose=1)\n",
    "        gs_clf.fit(X_train, y_train)\n",
    "    \n",
    "    train_acc = gs_clf.best_score_\n",
    "    \n",
    "    best_model = gs_clf.best_estimator_\n",
    "    dev_acc = best_model.score(X_dev, y_dev)\n",
    "    return train_acc, dev_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Напишите цикл обучения модели по эпохам.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем все данные, что у нас есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 0 1 1] 15000\n",
      "[0 0 1 ... 1 1 0] 10000\n"
     ]
    }
   ],
   "source": [
    "# Тексты для обучения doc2vec\n",
    "transductive_inds_dataset\n",
    "# Ещё нужны лейблы для обучения логистической регрессии.\n",
    "train_labels = np.array([int(lab == 'pos') for text, lab in train_dataset], dtype=np.int32)\n",
    "print(train_labels, len(train_labels))\n",
    "\n",
    "dev_labels = np.array([int(lab == 'pos') for text, lab in dev_dataset], dtype=np.int32)\n",
    "print(dev_labels, len(dev_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(doc2vec_model, d2v_data, vocab_probs, train_labels, dev_labels, num_epochs, doc2vec_lr):\n",
    "    train_accs = []\n",
    "    dev_accs = []\n",
    "    \n",
    "    \n",
    "    d2v_nb = 5\n",
    "    d2v_batch_size = 100\n",
    "    \n",
    "    total_epoch_iterations = ((d2v_nb + 1) * len(d2v_data[0])) // d2v_batch_size\n",
    "    \n",
    "    total_iter = num_epochs * total_epoch_iterations\n",
    "    \n",
    "    loss_stat_border = 150000\n",
    "    cur_iter = 0\n",
    "    for ep in range(num_epochs):\n",
    "        print(\"Start epoch #\", ep + 1)\n",
    "        print(\"Training Doc2Vec\")\n",
    "        batch_gen = batch_generator(*d2v_data, probs=vocab_probs)\n",
    "        avg_loss = 0.0\n",
    "        for ind, (word_inds, doc_inds, labels) in enumerate(tqdm(batch_gen)):\n",
    "            new_lr = doc2vec_lr * (1 - cur_iter * 1.0 / total_iter)\n",
    "            batch_loss = doc2vec.train(word_inds, doc_inds, labels, new_lr)\n",
    "            cur_iter += 1\n",
    "            avg_loss += batch_loss\n",
    "            if ind % loss_stat_border == 0 and ind != 0:\n",
    "                tqdm.write(f\"avg_loss: {avg_loss / loss_stat_border}\")\n",
    "                avg_loss = 0.0\n",
    "            \n",
    "        print(\"Starting logistic regression\")\n",
    "        \n",
    "        X_train, X_dev, X_test = doc2vec_model.get_all_X()\n",
    "        \n",
    "        t_acc, d_acc = train_logreg(X_train, train_labels, X_dev, dev_labels)\n",
    "        print(\"Train acc = \", t_acc, \" Dev acc =  \", d_acc)\n",
    "        train_accs.append(t_acc)\n",
    "        dev_accs.append(d_acc)\n",
    "    return train_accs, dev_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = Doc2Vec(len(w2ind), len(transductive_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start epoch # 1\n",
      "Training Doc2Vec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a8c084fdcb441e996ee824ce3c009b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 69.31516603481327\n",
      "avg_loss: 69.30763717463344\n",
      "\n",
      "Starting logistic regression\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   21.6s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc =  0.5916666666666667  Dev acc =   0.5997\n",
      "Start epoch # 2\n",
      "Training Doc2Vec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b4e3753f3d42029fc3e7460cffd83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 68.07554473198431\n",
      "avg_loss: 63.77839805013095\n",
      "\n",
      "Starting logistic regression\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc =  0.7155333333333334  Dev acc =   0.7169\n",
      "Start epoch # 3\n",
      "Training Doc2Vec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3391ef2e4340457c9ec2d9ed4b846353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 57.49392091619331\n",
      "avg_loss: 53.13841046058573\n",
      "\n",
      "Starting logistic regression\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.9s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:  7.0min finished\n",
      "/home/rmnnsk/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C is on border!\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:  8.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc =  0.7654  Dev acc =   0.7656\n",
      "Start epoch # 4\n",
      "Training Doc2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmnnsk/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd5a6908e0a46c3a54ab5138ca629eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 49.60746387181563\n",
      "avg_loss: 47.64303773843465\n",
      "\n",
      "Starting logistic regression\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed:  9.0min finished\n",
      "/home/rmnnsk/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C is on border!\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    3.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed: 10.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc =  0.7950666666666667  Dev acc =   0.7926\n",
      "Start epoch # 5\n",
      "Training Doc2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmnnsk/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8e1f0602b848698a70f20d64737daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 46.103554312859046\n",
      "avg_loss: 45.14614320294521\n",
      "\n",
      "Starting logistic regression\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed: 10.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc =  0.8174666666666667  Dev acc =   0.8148\n",
      "Start epoch # 6\n",
      "Training Doc2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmnnsk/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d172dd50a41543d29f87bbf821a24d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 44.32933396087691\n",
      "avg_loss: 43.734775207944786\n",
      "\n",
      "Starting logistic regression\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed: 12.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc =  0.8318666666666668  Dev acc =   0.8307\n",
      "Start epoch # 7\n",
      "Training Doc2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmnnsk/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa27ab6af8294fbeada35dbc5079d90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 43.205131392386974\n",
      "avg_loss: 42.779361674851174\n",
      "\n",
      "Starting logistic regression\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    8.8s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed: 12.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc =  0.8398  Dev acc =   0.8411\n",
      "Start epoch # 8\n",
      "Training Doc2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmnnsk/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91f6b4b4ff2491584e6daf0e8608725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 42.39308073389108\n",
      "avg_loss: 42.095640188898116\n",
      "\n",
      "Starting logistic regression\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    9.0s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed: 11.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc =  0.8447333333333333  Dev acc =   0.8456\n",
      "Start epoch # 9\n",
      "Training Doc2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmnnsk/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f17cfbcbb1b44668ad518fa9a9b8a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 41.813451444511806\n",
      "avg_loss: 41.602516138933524\n",
      "\n",
      "Starting logistic regression\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed: 10.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc =  0.8482  Dev acc =   0.8479\n",
      "Start epoch # 10\n",
      "Training Doc2Vec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmnnsk/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c145dfa23be4483a9739984f66245dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_loss: 41.44936494621847\n",
      "avg_loss: 41.34247249289598\n",
      "\n",
      "Starting logistic regression\n",
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done 300 out of 300 | elapsed: 10.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc =  0.8487333333333332  Dev acc =   0.8494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmnnsk/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "accs = fit(doc2vec, transductive_inds_dataset, vocab_tf_prob, train_labels, dev_labels, num_epochs=10, doc2vec_lr=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fafa0290490>]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.plot(range(10), accs[0])\n",
    "ax1.plot(range(10), accs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что точность на тренировочном и валидационном датасетах растёт одинаково и упирается в 85%.<br>\n",
    "Это говорит о том, что модель не переобучается и если добавить в неё побольше данных, то можно выбить больше точности (так и оказалось)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.  Опишите, какие гиперпараметры модели и каким образом подбирались? Какая получилась итоговая точность на train/dev/test выборках? Как зависит точность от значения гиперпараметров?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подбирались:\n",
    "<br><br>\n",
    "**1. Размер эмбеддинга из нескольких значений: [100, 300, 500]**\n",
    "<br>\n",
    "Из опыта предыдущего семестра было взято значение 500, т.к. классификатор с GloVe векторами давал наилучшие показатели именно с 500-мерными эмбедингами. \n",
    "Ну и плюс к этому обучать остальные модели было бы так же долго, а т.к. это было почти сразу обречено на провал, то было принятно решение этого не делать.\n",
    "<br><br>\n",
    "**2. learning_rate и количество эпох для обучения модели Doc2Vec**\n",
    "<br>\n",
    "Изначально, прочитав статью, я поставил **lr = 0.25** и градиенты взрывались на середине 1 эпохи.<br>\n",
    "Уменьшил до **lr = 0.15** и градиенты стали взрываться на второй эпохе (невероятно).<br>\n",
    "\n",
    "Потом я прочитал код на гитхабе, который прилагался к статье и оказалось, что там **lr=0.025**. (спасибо за счастливые 3-4 часа отладки правильного кода)<br>\n",
    "\n",
    "Но даже после этого градиенты взрывались на 10 эпохе и было принято решение сделать уменьшение лёрнинг рейта по мере обучения.<br>\n",
    "***Пока это линейное уменьшение от 0.03 до 0 на каждом шаге градиентного спуска.***\n",
    "<br><br>\n",
    "**3. Параметры для GridSearchCV:**\n",
    "<br>\n",
    "    Тут всё подбирается так, чтобы после обучения векторов документов наша линейная регрессия сходилась и делала это не за миллион лет.<br>\n",
    "    \n",
    "   **3.1. Количество различных С.**\n",
    "    <br>\n",
    "    Тут подбираем по количеству свободного времени. Я поставил от $10^{-3}$ до $10^3$ и $30$ различных значений.<br>\n",
    "    Такая большая граница сверху, чтобы С точно никогда не было на границе. И так и происходит. Но если будет на границе, то мы их увеличиваем и запускаем GridSearch ещё раз.\n",
    "    <br><br>\n",
    "    **3.2. Количество итераций логистической регрессии.**\n",
    "    <br>\n",
    "        Тут я столкнулся с проблемой, что на 50-100 итерациях регрессия не успевает сходиться. <br>\n",
    "        Поэтому приходится увеличивать их количество. В этом ноутбуке это 200-300 итераций, т.к. тут **GridSearch** запускается после каждой эпохи. В скрипте классификатора это 1000 итераций, т.к. запускается один раз и нужно, чтобы точно сошлось\n",
    "<br><br>\n",
    "\n",
    "**4.Параметры словаря: максимальные и минимальные частоты встречаемости, максимальный размер словаря.**\n",
    "<br>\n",
    "Тут исходим из такой логики: \n",
    "1. Мы хотим, чтобы слово встречалось как минимум в двух документах, потому что иначе оно не придаст никакого смысла модели (т.к. иначе это может быть любое рандомное слово).\n",
    "2. Мы хотим, чтобы слово встречалось в корпусе минимум 2-3 раза, чтобы отфильтровать опечатки.\n",
    "3. Мы хотим, чтобы остались триграммы, ведь они дают очень много смысла, но очень редко встречаются. Поэтому нужно аккуратнее ставить границы.\n",
    "4. Мы не хотим вечность ждать обучения.\n",
    "5. У нас ограниченный(хоть и большой) объем памяти.\n",
    "<br><br>\n",
    "**Из всего этого мы получаем следующие ограничения:**\n",
    "<br>\n",
    "Для ноутбука (тут нет unlabeled текстов и маленьких текстов): $min\\_tf=1, min\\_df=1, max\\_df=0.7, max\\_tokens=10^6$\n",
    "<br>\n",
    "**Всего $10^6$ уникальных токенов.**\n",
    "<br><br>\n",
    "Для оценивающего скрипта: $min\\_tf=3, min\\_df=3, max\\_df=0.5, max\\_tokens=2 * 10^6$\n",
    "<br>\n",
    "**Всего $1.5 * 10^6$ уникальных токенов.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Вопрос. Какие трудности возникли при выполнения задания?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Основная трудность в ожидании обучения. Это сильно затягивает исправление возможных ошибок и замедляет разработку новых фичей.\n",
    "\n",
    "2. Не очень информативная основная статья С ОПЕЧАТКАМИ. Но это компенсируется реально подробным заданием.\n",
    "\n",
    "3. Ещё хотелось бы знать хотя бы примерную желаемую скорость работы, чтобы было на что ориентироваться при написании и когда можно остановиться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше попробуем улучшить точность в **research** части."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
