{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1. Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теоретическая часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. \n",
    "Пусть, вы обучаетесь алгоритмом SGD и на данном шаге из датасета вы выбрали слово w и документ d. Тогда у вас есть один вектор слова $v_w$ и один вектор документа $v_d$, а также метка $y_i$, которая равна 1 в случае, если слово w содержится в документе d, и 0, если не содержится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**а) Выпишите функцию потерь для этой пары в схеме skip-gram negative sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(w, d) = -y_i * log(\\sigma(v_w \\cdot v_d)) - (1 - y_i) * log(1 - \\sigma(v_w \\cdot v_d))$$\n",
    "<br>\n",
    "где $a \\cdot b$ - скалярное произведение векторов $a$ и $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим $\\sigma(v_w \\cdot v_d)$ за $a$. Тогда \n",
    "$$L(w, d) = -y_i * log(a) - (1 - y_i) * log(1 - a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Найдите градиент по весам $v_w$ в терминах матричных операций.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial v_w} \n",
    "= \\frac{\\partial L}{\\partial a} * \\frac{\\partial a}{\\partial v_w} \n",
    "= \\frac{-y_i}{a} \\frac{\\partial a}{\\partial v_w} - \\frac{1 - y_i}{1 - a} \\frac{\\partial (1 - a)}{\\partial v_w}\n",
    "= \\frac{-y_i}{a} \\frac{\\partial a}{\\partial v_w} + \\frac{1 - y_i}{1 - a} \\frac{\\partial a}{\\partial v_w}\n",
    "= \\frac{\\partial a}{\\partial v_w} * \\left(\\frac{-y_i + y_i*a + a - y_i * a}{a * (1 - a)} \\right) =\n",
    "$$\n",
    "<br>\n",
    "$$ = \\frac{\\partial a}{\\partial v_w} \\frac{a - y_i}{a * (1 - a)} \n",
    "   = \\frac{a - y_i}{a * (1 - a)} * a * (1 - a) * \\frac{\\partial (v_w \\cdot v_d)}{\\partial v_w}\n",
    "   = (a - y_i) * v_d\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**с) Найдите градиент по весам vd в терминах матричных операций.**\n",
    "<br>\n",
    "<br>\n",
    "Т.к. $L$ симметрично относительно $v_w$ и $v_d$, то \n",
    "$$\\frac{\\partial L}{\\partial v_d} = (a -  y_i) * v_w$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.\n",
    "**а) Выпишите размерности матриц векторов слов и векторов документов**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть мы выбрали размер эмбеддингов **emb_size**. Тогда размеры матриц эмбеддингов будут равны: <br>\n",
    "**1) words_emb - [V x emb_size]** \n",
    "<br>\n",
    "**2) docs_emb - [D x emb_size]**\n",
    "\n",
    "Где **V** - размер составленного словаря из датасета, **D** - размер датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Выпишите размерности матриц после взятия подмножеств векторов отвечающих\n",
    "словам и документам из одного батча.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть размер батча - **batch_size**. Тогда размеры батчей слов и документов: \n",
    "<br>\n",
    "**1) words_emb - [batch_size x emb_size]** \n",
    "<br>\n",
    "**2) docs_emb - [batch_size x emb_size]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Выпишите размерность вектора скалярных произведений, вектора потерь.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размерность вектора скалярных произведений **logits** и вектора потерь (значения функции потерь для каждого примера из батча):\n",
    "\n",
    "**1) logits, losses - [batch_size x 1]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим случай, когда эмбединги слов обучаются по схеме word2vec skip-gram negative sampling, но отрицательные примеры не используются (то есть всем примерам из датасета соответствуют единичные метки). \n",
    "<br>\n",
    "**Вопрос: Как вам кажется, что будет происходить с векторами во время обучения?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы не используем отрицательные примеры для обучения, то нам не нужно уменьшать скалярное произведение эмбеддингов слов, которые не встречаются рядом(и видимо не близки по смыслу).\n",
    "<br>\n",
    "\n",
    "Значит мы можем безболезненно уменьшать лосс путем **бесконечного увеличения модулей всех векторов** -> увеличения их скалярного произведения. \n",
    "<br>\n",
    "Таким образом получаем, что возможно некоторые вектора и повернутся в процессе обучения, но это не будет нести почти никакой пользы, ведь все вектора будут очень большими по модулю, а значит похожими.\n",
    "<br>\n",
    "\n",
    "Поэтому, чтобы угол между векторами тоже нёс в себе какой-то смысл, в обучении нам нужны отрицательные примеры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. \n",
    "Предположим, что мы хотим решать задачу классификации текстов на основе их эмбедингов. При этом у нас есть три корпуса данных: обучающая, валидационная и тестовая выборка. \n",
    "<br>\n",
    "**Сколько моделей DV-ngram нужно построить для получения эмбедингов?\n",
    "<br>\n",
    "Можно ли использовать модель DV-ngram в режиме online (когда постоянно приходят\n",
    "новые тестовые данные)?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Казалось бы, что если мы решаем задачу классификации, то не хочется, чтобы данные из валидационного и тестового датасета использовались при обучении.\n",
    "<br>\n",
    "Но т.к. для обучения эмбеддингов не используются метки, то и в реальных задачах мы можем обучать их на неразмеченных данных. Значит для получения эмбеддингов **можно использовать все 3 выборки и строить только одну модель для всех сразу**. \n",
    "<br>\n",
    "Такой подход называется transductive learning и у него есть некоторые недостатки. Например то, что **мы не можем обучать эмбеддинги в режиме онлайн, потому что нарушается связь эмбеддингов документов с уже обученными эмбеддингами слов**. Поэтому для добавления новых тестовых данных нужно  полностью переобучать всю модель.\n",
    "<br>\n",
    "<br>\n",
    "Однако существуют некоторые подходы для **online** использования/обучения. Например можно фиксировать уже обученные вектора слов и на основе этих векторов и новых добавленных векторов слов обучать вектора предложений."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
